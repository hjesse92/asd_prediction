{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1P6CF3uM4xNDAq5dTcizGZYC0nntize9Z","timestamp":1700368770123},{"file_id":"1WqdMWI_zc4rP5dwQJwC62bM-AUoVqA5Q","timestamp":1700365329042}],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPhV9rl/ptq80mB2zZgQnDh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"0RoWNSjGTxOa","executionInfo":{"status":"ok","timestamp":1700390207057,"user_tz":480,"elapsed":5675,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import pandas as pd\n","import math\n","import copy\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","from sklearn.metrics import precision_score, recall_score"]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    def split_heads(self, x):\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    def forward(self, Q, K, V, mask=None):\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output"],"metadata":{"id":"zUTNfLR8UC5J","executionInfo":{"status":"ok","timestamp":1700390207057,"user_tz":480,"elapsed":8,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class PositionWiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"],"metadata":{"id":"aj1z0ncqUGWW","executionInfo":{"status":"ok","timestamp":1700390207057,"user_tz":480,"elapsed":7,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_seq_length, d_model)\n","        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]"],"metadata":{"id":"lDd4hltZUJFm","executionInfo":{"status":"ok","timestamp":1700390207057,"user_tz":480,"elapsed":7,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        attn_output = self.self_attn(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x"],"metadata":{"id":"QsnPgJJCUNoM","executionInfo":{"status":"ok","timestamp":1700390207057,"user_tz":480,"elapsed":6,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        attn_output = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n","        x = self.norm2(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm3(x + self.dropout(ff_output))\n","        return x"],"metadata":{"id":"5Xrg6STSUQtU","executionInfo":{"status":"ok","timestamp":1700390207059,"user_tz":480,"elapsed":8,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n","        super(Transformer, self).__init__()\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","\n","        self.fc = nn.Linear(d_model, tgt_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def generate_mask(self, src, tgt):\n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n","        seq_length = tgt.size(1)\n","        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n","        tgt_mask = tgt_mask & nopeak_mask\n","        return src_mask, tgt_mask\n","\n","    def forward(self, src, tgt):\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","\n","        enc_output = src_embedded\n","        for enc_layer in self.encoder_layers:\n","            enc_output = enc_layer(enc_output, src_mask)\n","\n","        dec_output = tgt_embedded\n","        for dec_layer in self.decoder_layers:\n","            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n","\n","        output = self.fc(dec_output)\n","        return output"],"metadata":{"id":"mmJBAkmwUUA4","executionInfo":{"status":"ok","timestamp":1700390207059,"user_tz":480,"elapsed":8,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive, files\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72CPulDZphMA","executionInfo":{"status":"ok","timestamp":1700390229470,"user_tz":480,"elapsed":22418,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}},"outputId":"3c2f7124-0064-4886-d600-fae0a72e7173"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Set your custom dataset path\n","data_dir = \"gdrive/MyDrive/Colab Notebooks/ucb_capstone_asd_detection/data_transformer/\"\n","input_file = 'input.pt'\n","label_file = 'labels.pt'\n","\n","# Create the full file path\n","input_path = f\"{data_dir}/{input_file}\"\n","label_path = f\"{data_dir}/{label_file}\"\n","\n","# Read the CSV file into a DataFrame\n","input = torch.load(input_path)\n","label = torch.load(label_path)"],"metadata":{"id":"s502_iRcp97i","executionInfo":{"status":"ok","timestamp":1700390231914,"user_tz":480,"elapsed":2449,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["label = torch.where(label != 0, torch.tensor(1), label)"],"metadata":{"id":"kiet3uQznBJh","executionInfo":{"status":"ok","timestamp":1700390231915,"user_tz":480,"elapsed":5,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Combine input and label into a TensorDataset\n","dataset = TensorDataset(input, label)\n","\n","# Specify the percentage of data to be used for validation (e.g., 20%)\n","validation_percentage = 0.2\n","val_size = int(len(dataset) * validation_percentage)\n","train_size = len(dataset) - val_size\n","\n","# Randomly split the dataset into training and validation sets\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# Create DataLoader for training and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=25, shuffle=False)"],"metadata":{"id":"X2HcZMnk69Jr","executionInfo":{"status":"ok","timestamp":1700390231915,"user_tz":480,"elapsed":4,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["input_vocab_size = 5000\n","label_vocab_size = 2\n","d_model = 512\n","num_heads = 8\n","num_layers = 6\n","d_ff = 2048\n","max_seq_length = 100\n","dropout = 0.1\n","\n","transformer = Transformer(input_vocab_size, label_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n"],"metadata":{"id":"SrHZGiJZUYIa","executionInfo":{"status":"ok","timestamp":1700390232434,"user_tz":480,"elapsed":523,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"],"metadata":{"id":"nvp7mL0qUdAF","executionInfo":{"status":"ok","timestamp":1700390234157,"user_tz":480,"elapsed":1726,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def calculate_accuracy(predictions, targets):\n","    \"\"\"\n","    Calculate accuracy given the predicted and target labels.\n","\n","    Args:\n","    - predictions (torch.Tensor): Predicted labels\n","    - targets (torch.Tensor): True labels\n","\n","    Returns:\n","    - accuracy (float): Accuracy value between 0 and 1\n","    \"\"\"\n","    with torch.no_grad():\n","        #_, predicted = torch.max(predictions, 1)\n","        predicted = torch.where(torch.argmax(predictions.squeeze(dim=1), dim=1) == 0, 1, 0)\n","        #predicted = torch.argmax(predictions, dim=1)\n","        correct = (predicted == targets).sum().item()\n","        total = targets.size(0)\n","        accuracy = correct / total\n","    return accuracy"],"metadata":{"id":"T143xLzd40oK","executionInfo":{"status":"ok","timestamp":1700390234157,"user_tz":480,"elapsed":3,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["for epoch in range(5):\n","\n","  # Training\n","  transformer.train()\n","  for batch_idx, (train_input, train_label) in enumerate(train_dataloader):\n","      optimizer.zero_grad()\n","      output = transformer(train_input, train_label)\n","      loss = criterion(output.contiguous().view(-1, label_vocab_size), train_label.contiguous().view(-1))\n","      #loss = criterion((torch.where(torch.argmax(output.squeeze(dim=1), dim=1) == 0, 1, 0)).to(dtype=torch.float), train_label.contiguous().view(-1))\n","      loss.backward()\n","      optimizer.step()\n","\n","  # Validation\n","  transformer.eval()\n","  val_loss = 0.0\n","  val_accuracy = 0\n","\n","  with torch.no_grad():\n","    for batch_idx, (val_input, val_label) in enumerate(val_dataloader):\n","      val_output = transformer(val_input, val_label)\n","      val_loss += criterion(val_output.contiguous().view(-1, label_vocab_size), val_label.contiguous().view(-1))\n","      #val_loss += criterion(torch.where(torch.argmax(val_output.squeeze(dim=1), dim=1) == 0, 1, 0), val_label.contiguous().view(-1))\n","\n","      # Calculate accuracy (you need to define an appropriate accuracy function)\n","      val_accuracy += calculate_accuracy(val_output, val_label)\n","\n","    val_loss = val_loss / len(val_dataloader)\n","    val_accuracy = val_accuracy / len(val_dataloader)\n","\n","    # Convert tensors to NumPy arrays\n","    #y_true_np = val_label.numpy()\n","    #y_pred_np = torch.where(torch.argmax(val_output.squeeze(dim=1), dim=1) == 0, 1, 0).numpy()\n","\n","      # Calculate precision and recall\n","     # precision = precision_score(y_true_np, y_pred_np)\n","      #recall = recall_score(y_true_np, y_pred_np)\n","\n","  print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}, Validation Accuracy: {val_accuracy}\")\n","  # Print the results\n","  #print(f'Precision: {precision:.4f}')\n","  #print(f'Recall: {recall:.4f}')\n","  #print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8BtrVmCAg73","executionInfo":{"status":"ok","timestamp":1700390356138,"user_tz":480,"elapsed":121984,"user":{"displayName":"Waqas Ali","userId":"05706414843926367914"}},"outputId":"2712d7df-1baf-4b4a-814a-4ef62e30c01b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 3.484578883217182e-07, Validation Loss: 2.3841855067985307e-07, Validation Accuracy: 12.4\n","Epoch: 2, Training Loss: 1.1920928244535389e-07, Validation Loss: 1.1920927533992653e-07, Validation Accuracy: 12.4\n","Epoch: 3, Training Loss: 1.1920927533992653e-07, Validation Loss: 1.1920927533992653e-07, Validation Accuracy: 12.4\n","Epoch: 4, Training Loss: 6.357828397085541e-08, Validation Loss: 0.0, Validation Accuracy: 12.4\n","Epoch: 5, Training Loss: 0.0, Validation Loss: 0.0, Validation Accuracy: 12.4\n"]}]}]}